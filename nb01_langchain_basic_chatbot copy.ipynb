{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682c4c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe84af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e461e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f93f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Not much! Just here and ready to chat. Is there something on your mind that you'd like to talk about or ask about? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 38, 'total_tokens': 71, 'completion_time': 0.076285897, 'prompt_time': 0.010673935, 'queue_time': 0.050966215, 'total_time': 0.086959832}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--27b069ad-6ee0-4483-ada0-032a54a7ce78-0', usage_metadata={'input_tokens': 38, 'output_tokens': 33, 'total_tokens': 71})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple model call\n",
    "\n",
    "model.invoke(\"Whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd459f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42416cbd",
   "metadata": {},
   "source": [
    "[Exercise] Play along. Give bigger and bigger prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2738af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 39, 'total_tokens': 62, 'completion_time': 0.021668612, 'prompt_time': 0.010686325, 'queue_time': 0.050440474, 'total_time': 0.032354937}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8811731b-9e3a-4ece-bf43-9d504d51e1a1-0', usage_metadata={'input_tokens': 39, 'output_tokens': 23, 'total_tokens': 62})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f4578d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Matrix Multiplication Code**\\n================================\\n\\n### Overview\\n\\nThis Python code performs matrix multiplication on two input matrices. It checks if the dimensions of the matrices are compatible for multiplication and throws an exception if they are not.\\n\\n### Code\\n```python\\nimport numpy as np\\n\\ndef multiply_matrices(mat1, mat2):\\n    \"\"\"\\n    Multiply two matrices.\\n\\n    Args:\\n        mat1 (list of lists): The first matrix.\\n        mat2 (list of lists): The second matrix.\\n\\n    Returns:\\n        list of lists: The product of the two matrices.\\n\\n    Raises:\\n        ValueError: If the dimensions of the matrices are not compatible for multiplication.\\n    \"\"\"\\n    # Get the dimensions of the matrices\\n    rows_mat1 = len(mat1)\\n    cols_mat1 = len(mat1[0])\\n    rows_mat2 = len(mat2)\\n    cols_mat2 = len(mat2[0])\\n\\n    # Check if the dimensions are compatible for multiplication\\n    if cols_mat1 != rows_mat2:\\n        raise ValueError(\"Matrix dimensions are not compatible for multiplication\")\\n\\n    # Initialize the result matrix with zeros\\n    result = [[0 for _ in range(cols_mat2)] for _ in range(rows_mat1)]\\n\\n    # Perform the matrix multiplication\\n    for i in range(rows_mat1):\\n        for j in range(cols_mat2):\\n            for k in range(cols_mat1):\\n                result[i][j] += mat1[i][k] * mat2[k][j]\\n\\n    return result\\n\\ndef main():\\n    # Example usage\\n    mat1 = [[1, 2, 3], [4, 5, 6]]\\n    mat2 = [[7, 8], [9, 10], [11, 12]]\\n\\n    try:\\n        result = multiply_matrices(mat1, mat2)\\n        print(\"Result:\")\\n        for row in result:\\n            print(row)\\n    except ValueError as e:\\n        print(e)\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation\\n\\n1. The `multiply_matrices` function takes two matrices `mat1` and `mat2` as input and returns their product.\\n2. It first checks if the dimensions of the matrices are compatible for multiplication by comparing the number of columns in `mat1` with the number of rows in `mat2`. If they are not compatible, it raises a `ValueError`.\\n3. If the dimensions are compatible, it initializes a result matrix with zeros and performs the matrix multiplication using nested loops.\\n4. The `main` function demonstrates the usage of the `multiply_matrices` function with an example.\\n\\n### Alternative Solution using NumPy\\n\\nYou can also use the NumPy library to perform matrix multiplication, which provides a more concise and efficient solution:\\n```python\\nimport numpy as np\\n\\ndef multiply_matrices(mat1, mat2):\\n    try:\\n        return np.matmul(mat1, mat2).tolist()\\n    except ValueError as e:\\n        raise ValueError(\"Matrix dimensions are not compatible for multiplication\") from e\\n\\ndef main():\\n    # Example usage\\n    mat1 = [[1, 2, 3], [4, 5, 6]]\\n    mat2 = [[7, 8], [9, 10], [11, 12]]\\n\\n    try:\\n        result = multiply_matrices(mat1, mat2)\\n        print(\"Result:\")\\n        for row in result:\\n            print(row)\\n    except ValueError as e:\\n        print(e)\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\nThis solution uses the `np.matmul` function to perform the matrix multiplication and converts the result to a list of lists using the `tolist` method.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 747, 'prompt_tokens': 68, 'total_tokens': 815, 'completion_time': 1.254872552, 'prompt_time': 0.01295729, 'queue_time': 0.05119086, 'total_time': 1.267829842}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4b6172cf-d012-46a4-99b3-c062eb3deb46-0', usage_metadata={'input_tokens': 68, 'output_tokens': 747, 'total_tokens': 815})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"\"\" Write a python code which can multiply two matrix of arbitrary but compatible order. The code should throw exception if the matrix dimentsions are not compatible for multiplication             \n",
    "             \"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c8c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Matrix Multiplication Code**\n",
      "================================\n",
      "\n",
      "### Overview\n",
      "\n",
      "This Python code performs matrix multiplication on two input matrices. It checks if the dimensions of the matrices are compatible for multiplication and throws an exception if they are not.\n",
      "\n",
      "### Code\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def multiply_matrices(mat1, mat2):\n",
      "    \"\"\"\n",
      "    Multiply two matrices.\n",
      "\n",
      "    Args:\n",
      "        mat1 (list of lists): The first matrix.\n",
      "        mat2 (list of lists): The second matrix.\n",
      "\n",
      "    Returns:\n",
      "        list of lists: The product of the two matrices.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the dimensions of the matrices are not compatible for multiplication.\n",
      "    \"\"\"\n",
      "    # Get the dimensions of the matrices\n",
      "    rows_mat1 = len(mat1)\n",
      "    cols_mat1 = len(mat1[0])\n",
      "    rows_mat2 = len(mat2)\n",
      "    cols_mat2 = len(mat2[0])\n",
      "\n",
      "    # Check if the dimensions are compatible for multiplication\n",
      "    if cols_mat1 != rows_mat2:\n",
      "        raise ValueError(\"Matrix dimensions are not compatible for multiplication\")\n",
      "\n",
      "    # Initialize the result matrix with zeros\n",
      "    result = [[0 for _ in range(cols_mat2)] for _ in range(rows_mat1)]\n",
      "\n",
      "    # Perform the matrix multiplication\n",
      "    for i in range(rows_mat1):\n",
      "        for j in range(cols_mat2):\n",
      "            for k in range(cols_mat1):\n",
      "                result[i][j] += mat1[i][k] * mat2[k][j]\n",
      "\n",
      "    return result\n",
      "\n",
      "def main():\n",
      "    # Example usage\n",
      "    mat1 = [[1, 2, 3], [4, 5, 6]]\n",
      "    mat2 = [[7, 8], [9, 10], [11, 12]]\n",
      "\n",
      "    try:\n",
      "        result = multiply_matrices(mat1, mat2)\n",
      "        print(\"Result:\")\n",
      "        for row in result:\n",
      "            print(row)\n",
      "    except ValueError as e:\n",
      "        print(e)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "1. The `multiply_matrices` function takes two matrices `mat1` and `mat2` as input and returns their product.\n",
      "2. It first checks if the dimensions of the matrices are compatible for multiplication by comparing the number of columns in `mat1` with the number of rows in `mat2`. If they are not compatible, it raises a `ValueError`.\n",
      "3. If the dimensions are compatible, it initializes a result matrix with zeros and performs the matrix multiplication using nested loops.\n",
      "4. The `main` function demonstrates the usage of the `multiply_matrices` function with an example.\n",
      "\n",
      "### Alternative Solution using NumPy\n",
      "\n",
      "You can also use the NumPy library to perform matrix multiplication, which provides a more concise and efficient solution:\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def multiply_matrices(mat1, mat2):\n",
      "    try:\n",
      "        return np.matmul(mat1, mat2).tolist()\n",
      "    except ValueError as e:\n",
      "        raise ValueError(\"Matrix dimensions are not compatible for multiplication\") from e\n",
      "\n",
      "def main():\n",
      "    # Example usage\n",
      "    mat1 = [[1, 2, 3], [4, 5, 6]]\n",
      "    mat2 = [[7, 8], [9, 10], [11, 12]]\n",
      "\n",
      "    try:\n",
      "        result = multiply_matrices(mat1, mat2)\n",
      "        print(\"Result:\")\n",
      "        for row in result:\n",
      "            print(row)\n",
      "    except ValueError as e:\n",
      "        print(e)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "This solution uses the `np.matmul` function to perform the matrix multiplication and converts the result to a list of lists using the `tolist` method.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b215da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='হাই!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 45, 'total_tokens': 51, 'completion_time': 0.00152905, 'prompt_time': 0.011401769, 'queue_time': 0.044739121, 'total_time': 0.012930819}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--44f70752-8cd4-4d52-8b5f-af6a29f071cf-0', usage_metadata={'input_tokens': 45, 'output_tokens': 6, 'total_tokens': 51})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoking to build a converstation style call\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Bengali\"), # try punjabi, or any other Indian language\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9efe5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"Generate python code for given tasks\"),\n",
    "    HumanMessage(\"Find max of given n numbers\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17d70ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Finding the Maximum of N Numbers in Python**\n",
      "====================================================\n",
      "\n",
      "Here's a simple Python function that finds the maximum of N numbers:\n",
      "\n",
      "```python\n",
      "def find_max(numbers):\n",
      "    \"\"\"\n",
      "    Find the maximum of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        numbers (list): A list of numbers.\n",
      "\n",
      "    Returns:\n",
      "        int: The maximum number in the list.\n",
      "    \"\"\"\n",
      "    return max(numbers)\n",
      "\n",
      "# Example usage:\n",
      "numbers = [12, 45, 7, 23, 56, 89, 34]\n",
      "max_number = find_max(numbers)\n",
      "print(\"Maximum number:\", max_number)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The `max()` function is a built-in Python function that returns the largest item in an iterable (like a list or tuple) or the largest of two or more arguments.\n",
      "* The `find_max()` function takes a list of numbers as input and returns the maximum number using the `max()` function.\n",
      "* In the example usage, we define a list of numbers and pass it to the `find_max()` function to find the maximum number.\n",
      "\n",
      "**Alternative Implementation (without using the `max()` function):**\n",
      "\n",
      "```python\n",
      "def find_max(numbers):\n",
      "    max_number = numbers[0]\n",
      "    for num in numbers[1:]:\n",
      "        if num > max_number:\n",
      "            max_number = num\n",
      "    return max_number\n",
      "\n",
      "# Example usage:\n",
      "numbers = [12, 45, 7, 23, 56, 89, 34]\n",
      "max_number = find_max(numbers)\n",
      "print(\"Maximum number:\", max_number)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* We initialize the `max_number` variable with the first number in the list.\n",
      "* We iterate through the rest of the list, and for each number, we check if it's greater than the current `max_number`.\n",
      "* If it is, we update the `max_number` variable with the new maximum number.\n",
      "* Finally, we return the `max_number` variable, which now holds the maximum number in the list.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36098aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Finding the Maximum of N Numbers in Python**\n",
      "====================================================\n",
      "\n",
      "Here's a simple Python function that finds the maximum of N numbers:\n",
      "\n",
      "```python\n",
      "def find_max(numbers):\n",
      "    \"\"\"\n",
      "    Find the maximum of a list of numbers.\n",
      "\n",
      "    Args:\n",
      "        numbers (list): A list of numbers.\n",
      "\n",
      "    Returns:\n",
      "        int: The maximum number in the list.\n",
      "    \"\"\"\n",
      "    return max(numbers)\n",
      "\n",
      "# Example usage:\n",
      "numbers = [12, 45, 7, 23, 56, 89, 34]\n",
      "max_number = find_max(numbers)\n",
      "print(\"Maximum number:\", max_number)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The `max()` function is a built-in Python function that returns the largest item in an iterable (like a list or tuple) or the largest of two or more arguments.\n",
      "* The `find_max()` function takes a list of numbers as input and returns the maximum number using the `max()` function.\n",
      "* In the example usage, we define a list of numbers and pass it to the `find_max()` function to find the maximum number.\n",
      "\n",
      "**Alternative Implementation (without using the `max()` function):**\n",
      "\n",
      "```python\n",
      "def find_max(numbers):\n",
      "    max_number = numbers[0]\n",
      "    for num in numbers[1:]:\n",
      "        if num > max_number:\n",
      "            max_number = num\n",
      "    return max_number\n",
      "\n",
      "# Example usage:\n",
      "numbers = [12, 45, 7, 23, 56, 89, 34]\n",
      "max_number = find_max(numbers)\n",
      "print(\"Maximum number:\", max_number)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* We initialize the `max_number` variable with the first number in the list.\n",
      "* We iterate through the rest of the list, and for each number, we check if it's greater than the current `max_number`.\n",
      "* If it is, we update the `max_number` variable with the new maximum number.\n",
      "* Finally, we return the `max_number` variable, which now holds the maximum number in the list.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39caa649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content='Hello' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content=' How' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content=' I' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content=' assist' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content=' today' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content='?' additional_kwargs={} response_metadata={} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand'} id='run--9f5a4462-6cb7-4092-a93f-0cc94668c5c6' usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46}\n"
     ]
    }
   ],
   "source": [
    "# streaming example\n",
    "import time\n",
    "\n",
    "for token in model.stream(\"hello\"):\n",
    "    time.sleep(0.1)\n",
    "    #print(token.content, end=\"|\")\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f083693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x10e29e560>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stream(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6837d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class discussion point: What is an LLM as a program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca534bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model1 = init_chat_model(\"meta-llama/llama-4-maverick-17b-128e-instruct\", model_provider=\"groq\")\n",
    "\n",
    "# exercise: Replace this model with some other model on the groq website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78c8fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a408f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Conversation:\n",
      "\n",
      "Model 1: Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n",
      "Model 2: It's nice to meet you too. I'm happy to chat with you about anything that's on your mind. I don't have personal needs or tasks, so I'm all ears (or rather, all text) and ready to engage in a conversation. What's been going on in your world lately? Would you like to talk about a specific topic, share a story, or just see where the conversation takes us?\n",
      "Model 1: It seems like you've started the conversation by responding to a greeting that wasn't actually there. Let's start fresh! I'm happy to chat with you. What would you like to talk about?\n",
      "Model 2: You're right, I sometimes anticipate a greeting. Let's start fresh indeed.\n",
      "\n",
      "I'm happy to chat with you about anything that interests you. If you're looking for ideas, I can suggest a few topics. We could discuss books, movies, or TV shows you've enjoyed recently. Or, if you're feeling curious, we could explore a random topic like space exploration, artificial intelligence, or a fascinating scientific discovery.\n",
      "\n",
      "What sounds appealing to you, or do you have something else on your mind that you'd like to talk about?\n",
      "Model 1: It seems like we started in the middle of a conversation. I'm happy to start fresh as well.\n",
      "\n",
      "To respond to your suggestions, I'm a large language model, so I don't have personal preferences or experiences like humans do, but I can certainly chat with you about a wide range of topics. I can discuss books, movies, or TV shows, or explore topics like space exploration, artificial intelligence, or scientific discoveries.\n",
      "\n",
      "If you're interested, I can also generate a random topic or question to get the conversation started. Alternatively, feel free to share something that's on your mind, and we can go from there.\n",
      "\n",
      "One possible topic could be: What are some of the most interesting or thought-provoking conversations you've had recently? I'm here to listen and respond.\n",
      "Model 2: It seems like we did start in the middle of a conversation. Thank you for clarifying and offering a fresh start.\n",
      "\n",
      "I think it's great that you're a large language model, as it allows us to have a wide-ranging conversation on various topics. I'd love to explore some of those topics you mentioned, such as space exploration, artificial intelligence, or scientific discoveries.\n",
      "\n",
      "Regarding your question about interesting conversations, I don't have personal experiences, but I can tell you that I've been trained on a vast amount of text data, which includes conversations on diverse topics. If you're interested, we could discuss some of the most thought-provoking or innovative ideas that have emerged in recent years.\n",
      "\n",
      "To get us started, I'll take you up on your offer to generate a random topic or question. Please go ahead and generate something, and we can take it from there. Alternatively, if you'd like to share something that's on your mind, I'm all ears (or rather, all text). Let's have a great conversation!\n",
      "Model 1: This is actually the start of our conversation, and I'm glad we can begin fresh.\n",
      "\n",
      "I'm excited to chat with you about a wide range of topics, from space exploration and artificial intelligence to scientific discoveries and beyond. I've been trained on a vast amount of text data, which enables me to respond to a diverse array of questions and prompts.\n",
      "\n",
      "Let's generate a random topic to get us started. Here's a question: \"What if humanity were to establish a self-sustaining colony on a distant planet? How might this change our understanding of the universe, our society, and ourselves?\"\n",
      "\n",
      "Feel free to share your thoughts, and we can explore this idea together. Alternatively, if you'd like to propose a different topic or ask a question, I'm here to respond and help guide the conversation.\n",
      "Model 2: What a fascinating topic to start with. Establishing a self-sustaining colony on a distant planet would be a groundbreaking achievement, with far-reaching implications for humanity. \n",
      "\n",
      "Initially, it would likely challenge our current understanding of the universe by providing new insights into the possibilities of life beyond Earth. The discovery of exoplanets and the potential for supporting life on other planets would raise questions about the uniqueness of Earth and the probability of intelligent life existing elsewhere in the universe. A human settlement on another planet would also offer a unique opportunity to study the effects of long-term space travel and the adaptation of humans to new environments.\n",
      "\n",
      "In terms of societal impact, a self-sustaining colony could lead to the development of new forms of governance, economy, and culture. The colony might need to establish its own systems for resource management, conflict resolution, and social organization, which could diverge from those on Earth. This could, in turn, lead to new perspectives on what it means to be human and how we interact with each other and our environment.\n",
      "\n",
      "On a personal level, the experience of living in a distant planet colony could profoundly change individuals' perceptions of themselves and their place in the universe. The isolation and challenges of living in a remote, alien environment could foster a sense of community and cooperation among colonists, as well as a deeper appreciation for the fragility and beauty of life.\n",
      "\n",
      "Some potential questions to explore further: What would be the most significant challenges in establishing a self-sustaining colony, and how might they be overcome? How might the experience of living in a distant planet colony influence the evolution of human culture, values, and identity? What would be the implications of a human colony on a distant planet for our understanding of the long-term survival and expansion of human civilization?\n",
      "\n",
      "I'd love to hear your thoughts on these questions and explore this topic further. Alternatively, if you'd like to introduce a new topic or question, I'm eager to engage in a conversation and see where it takes us.\n",
      "Model 1: I'm glad you're excited about the prospect of establishing a self-sustaining colony on a distant planet. Let's dive into some of the questions and ideas you've raised.\n",
      "\n",
      "First, the challenges in establishing a self-sustaining colony are numerous. Some of the most significant ones include:\n",
      "\n",
      "1. **Distance and Communication**: The vast distances between Earth and the colony would make communication a significant challenge. Signals would be delayed, and real-time communication might be impossible. This could lead to a sense of isolation and disconnection from Earth's events.\n",
      "2. **Resource Management**: The colony would need to be self-sufficient in terms of resources, such as food, water, and energy. This would require developing closed-loop life support systems, reliable renewable energy sources, and efficient waste management.\n",
      "3. **Radiation Protection**: Space is filled with harmful radiation, which could pose a significant risk to both the colonists and electronic equipment. Adequate shielding and protective measures would be essential.\n",
      "4. **Gravity and Health**: Prolonged exposure to microgravity or low-gravity environments could have unforeseen effects on the human body. The colony would need to develop strategies to mitigate these effects, such as rotating sections of the habitat to simulate gravity.\n",
      "5. **Psychological Factors**: The isolation and confinement of living in a distant planet colony could take a toll on the mental health of colonists. Strategies for maintaining mental well-being, such as virtual reality, social support networks, and recreational activities, would be crucial.\n",
      "\n",
      "To overcome these challenges, the colony might employ various strategies, such as:\n",
      "\n",
      "1. **In-situ Resource Utilization (ISRU)**: Using local resources to support the colony, such as extracting water from the planet's surface or atmosphere.\n",
      "2. **Closed-Loop Life Support Systems**: Implementing systems that recycle resources, minimize waste, and maintain a stable environment.\n",
      "3. **Advanced Technologies**: Developing and deploying cutting-edge technologies, such as advanced propulsion systems, reliable renewable energy sources, and sophisticated life support systems.\n",
      "4. **Robust Governance and Social Structures**: Establishing effective governance, social support networks, and conflict resolution mechanisms to maintain social cohesion and cooperation.\n",
      "\n",
      "The experience of living in a distant planet colony could significantly influence the evolution of human culture, values, and identity. For example:\n",
      "\n",
      "1. **New Perspectives on Human Existence**: Living in a distant planet colony could lead to a deeper appreciation for the fragility and beauty of life, as well as a greater understanding of the universe and our place within it.\n",
      "2. **Cultural Exchange and Evolution**: The colony might develop its own unique culture, shaped by the challenges and opportunities of living on a distant planet. This could lead to a rich cultural exchange between Earth and the colony.\n",
      "3. **Adaptation and Resilience**: The colony would need to adapt to the harsh environment and develop resilience in the face of challenges. This could lead to the development of new values and practices that prioritize sustainability, cooperation, and resourcefulness.\n",
      "\n",
      "The implications of a human colony on a distant planet for our understanding of the long-term survival and expansion of human civilization are significant. For example:\n",
      "\n",
      "1. **Ensuring Human Survival**: A self-sustaining colony could provide a safeguard against global catastrophes on Earth, ensuring the survival of human civilization.\n",
      "2. **Expanding Human Knowledge**: The colony could provide a unique opportunity for scientific research, exploration, and discovery, driving human knowledge and understanding forward.\n",
      "3. **New Opportunities for Human Settlement**: A successful colony could pave the way for further human settlement and expansion into the solar system and beyond.\n",
      "\n",
      "These are just a few thoughts on the topic. I'd love to explore these ideas further and discuss the many other implications and challenges associated with establishing a self-sustaining colony on a distant planet.\n",
      "\n",
      "What specific aspects of this topic would you like to explore further?\n",
      "Model 2: I'm glad you're enthusiastic about the idea of a self-sustaining colony on a distant planet. I'll provide a more structured response to facilitate our discussion.\n",
      "\n",
      "To begin with, I'd like to explore the **Resource Management** aspect of establishing a colony. You mentioned the importance of developing closed-loop life support systems, reliable renewable energy sources, and efficient waste management. I'd like to delve deeper into the specifics of these systems and how they could be implemented.\n",
      "\n",
      "For instance, what types of renewable energy sources would be most suitable for a distant planet colony? Would solar panels, wind turbines, or nuclear reactors be more effective? How could the colony develop a reliable and sustainable food supply, considering the challenges of growing crops in a controlled environment?\n",
      "\n",
      "Additionally, I'm interested in the **Psychological Factors** that could impact the colonists' mental health and well-being. You mentioned strategies like virtual reality, social support networks, and recreational activities. I'd like to discuss the potential role of artificial intelligence in supporting the colonists' mental health, such as AI-powered therapy sessions or AI-assisted social companions.\n",
      "\n",
      "Regarding the **Implications for Human Civilization**, I'd like to explore the idea of ensuring human survival through a self-sustaining colony. How could the colony be designed to withstand global catastrophes on Earth, and what measures could be taken to ensure the colony's continued survival and growth?\n",
      "\n",
      "Lastly, I'm intrigued by the potential for **Cultural Exchange and Evolution** between Earth and the colony. How might the colony's unique cultural identity develop over time, and what kinds of cultural exchange programs could be established to facilitate the sharing of ideas, values, and practices between the two?\n",
      "\n",
      "These are just a few aspects of the topic that I'd like to explore further. What are your thoughts on these ideas, and where would you like to take the discussion from here?\n"
     ]
    }
   ],
   "source": [
    "# Conversation between two models (model1 and model2)\n",
    "\n",
    "num_turns = 10\n",
    "message = \"Hello!\"\n",
    "print(\"Start of Conversation:\\n\")\n",
    "\n",
    "for i in range(num_turns):\n",
    "    if i % 2 == 0:\n",
    "        # model1 responding\n",
    "        response = model1.invoke(message)\n",
    "        print(f\"Model 1: {response.content}\")\n",
    "        message = response.content  # passing to next model\n",
    "    else:\n",
    "        # model2 responding\n",
    "        response = model2.invoke(message)\n",
    "        print(f\"Model 2: {response.content}\")\n",
    "        message = response.content  # passing to next model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49aac9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Teacher–Student Conversation:\n",
      "\n",
      "Teacher: Welcome to our lesson on Large Language Models (LLMs). I'm excited to guide you through this fascinating topic. To get started, let's break down what LLMs are and how they work.\n",
      "\n",
      "**What are Large Language Models?**\n",
      "\n",
      "Large Language Models are a type of artificial intelligence (AI) designed to process and understand human language. They're trained on vast amounts of text data, which enables them to learn patterns, relationships, and structures within language.\n",
      "\n",
      "**How do LLMs work?**\n",
      "\n",
      "LLMs are typically based on a type of neural network architecture called a transformer. This architecture is particularly well-suited for natural language processing tasks. Here's a simplified overview of how LLMs work:\n",
      "\n",
      "1. **Training Data**: LLMs are trained on massive datasets of text, which can include books, articles, websites, and more.\n",
      "2. **Tokenization**: The text data is broken down into smaller units called tokens, which can be words, subwords, or characters.\n",
      "3. **Model Architecture**: The transformer architecture is used to process the tokenized input data. This involves self-attention mechanisms that allow the model to weigh the importance of different tokens relative to each other.\n",
      "4. **Training Objective**: The model is trained to predict the next token in a sequence, given the context of the previous tokens. This is typically done using a masked language modeling objective, where some tokens are randomly masked, and the model is trained to predict the original token.\n",
      "5. **Model Outputs**: Once trained, LLMs can generate text, translate languages, summarize content, and perform other natural language processing tasks.\n",
      "\n",
      "**Key Characteristics of LLMs**\n",
      "\n",
      "Some key characteristics of LLMs include:\n",
      "\n",
      "* **Large-scale training data**: LLMs are trained on vast amounts of text data, often billions of words or more.\n",
      "* **Complex model architectures**: LLMs use sophisticated neural network architectures, such as transformers, to process and understand language.\n",
      "* **Ability to generate text**: LLMs can generate coherent and context-specific text, often indistinguishable from human-written content.\n",
      "\n",
      "Now that we've covered the basics, I'd like to ask: What specific aspects of LLMs would you like to explore further? Are you interested in their applications, training methods, or something else?\n",
      "\n",
      "Student: I'm really interested in learning more about LLMs, and I appreciate the detailed introduction. However, I have to admit that I'm a bit confused about the tokenization process and how the transformer architecture works.\n",
      "\n",
      "You mentioned that the text data is broken down into smaller units called tokens, which can be words, subwords, or characters. But how does the model decide which tokens to use? Is it based on the frequency of the words or the context in which they appear?\n",
      "\n",
      "Also, you mentioned that the transformer architecture uses self-attention mechanisms to weigh the importance of different tokens relative to each other. Can you elaborate on how this works? How does the model determine which tokens are more important than others, and what's the role of the self-attention mechanism in this process?\n",
      "\n",
      "Lastly, I'm intrigued by the idea that LLMs can generate text that's often indistinguishable from human-written content. How do LLMs achieve this level of coherence and context-specificity? Is it solely due to the large-scale training data, or are there other factors at play?\n",
      "\n",
      "I'd love to explore these topics further and gain a deeper understanding of how LLMs work. Perhaps we could delve into some examples or case studies to illustrate these concepts?\n",
      "\n",
      "Teacher: I'm glad you're interested in learning more about LLMs! Tokenization, transformer architecture, and text generation are all crucial aspects of these models, and I'm happy to break them down for you.\n",
      "\n",
      "**Tokenization**\n",
      "\n",
      "Tokenization is the process of breaking down text into smaller units, called tokens, that can be processed by the model. The goal is to represent the text in a way that's both meaningful and computationally efficient.\n",
      "\n",
      "The tokenization process typically involves a combination of the following steps:\n",
      "\n",
      "1. **Text preprocessing**: The text is cleaned and normalized, which includes converting all text to lowercase, removing punctuation, and handling special characters.\n",
      "2. **Tokenization algorithm**: A tokenization algorithm is applied to the preprocessed text. Common algorithms include:\n",
      "\t* WordPiece tokenization (used in BERT and other models): This algorithm breaks down words into subwords, which are smaller units that can be combined to form words. For example, \"unbreakable\" might be broken down into \"un\" + \"break\" + \"able\".\n",
      "\t* Byte-Pair Encoding (BPE): This algorithm iteratively merges the most frequent adjacent bytes in the text to form new tokens.\n",
      "\n",
      "The model doesn't directly decide which tokens to use based on frequency or context. Instead, the tokenization algorithm is typically trained on a large corpus of text data, and the resulting token vocabulary is fixed. The model's training data is then tokenized using this vocabulary.\n",
      "\n",
      "For example, if you're using a WordPiece tokenization algorithm, the model might learn to represent the word \"unbreakable\" as a sequence of subwords: [\"un\", \"##break\", \"##able\"]. The \"##\" symbol indicates that the subword is a continuation of the previous token.\n",
      "\n",
      "**Transformer Architecture and Self-Attention**\n",
      "\n",
      "The transformer architecture is a key component of LLMs, and it's based on self-attention mechanisms.\n",
      "\n",
      "Self-attention allows the model to weigh the importance of different tokens relative to each other. Here's a simplified overview of how it works:\n",
      "\n",
      "1. **Token representations**: Each token is represented as a vector, which is learned during training. These vectors capture the semantic meaning of the tokens.\n",
      "2. **Query, Key, and Value**: The token representations are used to compute three vectors: Query (Q), Key (K), and Value (V). These vectors are used to compute attention weights.\n",
      "3. **Attention weights**: The attention weights are computed by taking the dot product of Q and K, and applying a softmax function. This produces a probability distribution over the tokens, indicating their relative importance.\n",
      "4. **Weighted sum**: The attention weights are used to compute a weighted sum of the V vectors, which produces the final output.\n",
      "\n",
      "The self-attention mechanism allows the model to capture complex relationships between tokens, such as:\n",
      "\n",
      "* **Long-range dependencies**: The model can attend to tokens that are far apart in the input sequence, allowing it to capture long-range dependencies.\n",
      "* **Contextualized representations**: The model can learn contextualized representations of tokens, which take into account the surrounding tokens.\n",
      "\n",
      "**Text Generation**\n",
      "\n",
      "LLMs can generate coherent and context-specific text by leveraging the patterns and structures learned during training. Here are some key factors that contribute to their ability to generate high-quality text:\n",
      "\n",
      "1. **Large-scale training data**: LLMs are trained on vast amounts of text data, which allows them to learn a wide range of linguistic patterns and structures.\n",
      "2. **Contextualized representations**: The self-attention mechanism enables the model to learn contextualized representations of tokens, which helps to capture the nuances of language.\n",
      "3. **Generative capabilities**: LLMs are trained to predict the next token in a sequence, given the context. This allows them to generate text that's coherent and context-specific.\n",
      "\n",
      "To illustrate these concepts, let's consider an example. Suppose we want to generate text that's similar to a given prompt. We can use an LLM to generate a sequence of tokens that follows the prompt. The model will use its self-attention mechanism to weigh the importance of different tokens in the prompt, and generate text that's coherent and context-specific.\n",
      "\n",
      "For instance, if the prompt is \"The capital of France is\", the model might generate the text \"Paris, a city known for its stunning architecture and rich history.\" The model has learned to capture the context of the prompt, and generate text that's relevant and coherent.\n",
      "\n",
      "I'd be happy to explore more examples or case studies with you, or answer any further questions you may have!\n",
      "\n",
      "Student: I'm so glad you explained all this to me. I think I have a basic understanding of tokenization, transformer architecture, and text generation, but I'm still a bit confused about some of the details.\n",
      "\n",
      "Just to make sure I understand, tokenization is the process of breaking down text into smaller units called tokens, and there are different algorithms like WordPiece and BPE that can be used to do this. But how do these algorithms decide which tokens to use? You mentioned that the tokenization algorithm is typically trained on a large corpus of text data, but I'm not sure what that means.\n",
      "\n",
      "For example, if I'm using a WordPiece tokenization algorithm, how does it know to break down the word \"unbreakable\" into \"un\", \"##break\", and \"##able\"? Is it just based on the frequency of the word in the training data, or is there something more complex going on?\n",
      "\n",
      "And then there's the transformer architecture, which uses self-attention mechanisms to weigh the importance of different tokens. I think I understand the basic idea of self-attention, but I'm not sure how it's implemented in practice. You mentioned that the model computes three vectors: Query, Key, and Value, but I'm not sure what these vectors represent or how they're used to compute attention weights.\n",
      "\n",
      "Finally, I'm interested in learning more about how LLMs generate text. You mentioned that they're trained to predict the next token in a sequence, given the context, but I'm not sure how they actually generate coherent and context-specific text. Is it just a matter of predicting one token at a time, or is there something more complex going on?\n",
      "\n",
      "I'd love it if you could provide some more examples or case studies to illustrate these concepts, or answer any further questions I may have!\n",
      "\n",
      "Teacher: I'm glad you're eager to dive deeper into these concepts. I'll do my best to clarify the details and provide examples.\n",
      "\n",
      "### Tokenization Algorithms\n",
      "\n",
      "You're right that tokenization algorithms like WordPiece and BPE are trained on a large corpus of text data. The goal is to learn a vocabulary of subwords (smaller units within words) that can effectively represent the language.\n",
      "\n",
      "Let's take WordPiece as an example. When training a WordPiece model, the algorithm starts with a large corpus of text data and initializes a vocabulary with individual characters. Then, it iteratively merges adjacent characters or subwords to form new subwords based on their frequency in the corpus.\n",
      "\n",
      "The decision to merge two subwords is based on a score that reflects their frequency and the frequency of the resulting merged subword. The algorithm keeps merging subwords until it reaches a predetermined vocabulary size.\n",
      "\n",
      "In the case of the word \"unbreakable\", the WordPiece algorithm might break it down into \"un\", \"##break\", and \"##able\" because:\n",
      "\n",
      "1. \"un\" is a common prefix and appears frequently in the corpus.\n",
      "2. \"break\" is a common root word, and \"##break\" indicates that it's a continuation of a previous subword (the \"##\" notation is specific to WordPiece).\n",
      "3. \"able\" is a common suffix, and \"##able\" indicates that it's a continuation of the previous subword.\n",
      "\n",
      "The frequency of \"unbreakable\" as a whole word also plays a role, but it's not the only factor. The algorithm is trying to strike a balance between representing common subwords and preserving the meaning of the original word.\n",
      "\n",
      "### Transformer Architecture and Self-Attention\n",
      "\n",
      "The transformer architecture relies heavily on self-attention mechanisms to weigh the importance of different tokens in a sequence. To compute attention weights, the model uses three vectors: Query (Q), Key (K), and Value (V).\n",
      "\n",
      "These vectors are derived from the input embeddings (representations of the input tokens) through linear transformations. Think of them as different \"views\" of the input embeddings:\n",
      "\n",
      "* Query (Q): represents the context in which the attention is being computed.\n",
      "* Key (K): represents the information being attended to.\n",
      "* Value (V): represents the information that's being retrieved or used.\n",
      "\n",
      "The attention weights are computed by taking the dot product of Q and K, applying a scaling factor, and then passing the result through a softmax function. This produces a probability distribution over the input tokens, indicating their relative importance.\n",
      "\n",
      "The attention weights are then used to compute a weighted sum of the Value (V) vectors, producing the final output.\n",
      "\n",
      "To illustrate this, consider a simple example:\n",
      "\n",
      "Input sequence: \"The cat sat on the mat.\"\n",
      "\n",
      "When computing the representation for the token \"sat\", the model might attend to the surrounding tokens (\"cat\", \"on\", \"the\") to understand the context. The Query vector (Q) represents the context (\"sat\"), the Key vectors (K) represent the surrounding tokens, and the Value vectors (V) represent the information being retrieved.\n",
      "\n",
      "The attention weights would reflect the relative importance of each surrounding token, allowing the model to focus on the most relevant information.\n",
      "\n",
      "### Text Generation with LLMs\n",
      "\n",
      "LLMs generate text by predicting the next token in a sequence, given the context. However, it's not just a simple matter of predicting one token at a time.\n",
      "\n",
      "When generating text, the model uses the previously generated tokens as input to predict the next token. This process is often referred to as \"autoregressive\" generation.\n",
      "\n",
      "Here's a high-level overview of how it works:\n",
      "\n",
      "1. The model is given an initial prompt or context.\n",
      "2. The model generates the first token based on the prompt.\n",
      "3. The generated token is fed back into the model as input, along with the original prompt.\n",
      "4. The model generates the next token based on the updated input.\n",
      "5. Steps 3-4 are repeated until a stopping criterion is reached (e.g., a maximum sequence length).\n",
      "\n",
      "To generate coherent and context-specific text, LLMs rely on the following:\n",
      "\n",
      "* The model's ability to capture long-range dependencies and contextual relationships through self-attention mechanisms.\n",
      "* The model's training data, which includes a vast amount of text from various sources, allowing it to learn patterns and structures in language.\n",
      "\n",
      "While predicting one token at a time, the model is able to generate text that's coherent and context-specific because it's leveraging the patterns and structures it learned during training.\n",
      "\n",
      "### Example Case Study\n",
      "\n",
      "Let's consider a simple example: generating a short story based on a prompt.\n",
      "\n",
      "Prompt: \"In a world where magic was real\"\n",
      "\n",
      "The model might generate the following text:\n",
      "\n",
      "\"In a world where magic was real, the kingdom of Eldrador was renowned for its powerful sorcerers. The king, a just ruler, had appointed a council of wise wizards to govern the use of magic.\"\n",
      "\n",
      "Here's how the model might generate this text:\n",
      "\n",
      "1. It starts by generating the first token (\"In\") based on the prompt.\n",
      "2. It generates the next token (\"a\") based on the context (\"In\").\n",
      "3. It continues generating tokens, using the previously generated tokens as input, until it reaches the end of the sentence.\n",
      "\n",
      "Throughout this process, the model is using its self-attention mechanisms to weigh the importance of different tokens and capture contextual relationships.\n",
      "\n",
      "I hope this helps clarify the details! Do you have any further questions or would you like to explore more examples?\n",
      "\n",
      "Student: I think I'm starting to grasp the concepts, but I still have some questions to clarify my understanding.\n",
      "\n",
      "Regarding the WordPiece algorithm, you mentioned that it starts with individual characters and merges them into subwords based on frequency. How does it decide when to stop merging subwords? Is it solely based on the predetermined vocabulary size, or are there other factors at play?\n",
      "\n",
      "Additionally, I'm curious about the role of the \"##\" notation in WordPiece. You mentioned that it indicates a continuation of a previous subword, but how does the model know when to use this notation? Is it learned during training, or is it a predefined rule?\n",
      "\n",
      "Moving on to the transformer architecture, I'd like to delve deeper into the self-attention mechanism. You explained that the attention weights are computed by taking the dot product of Q and K, applying a scaling factor, and then passing the result through a softmax function. Can you elaborate on the purpose of the scaling factor? Is it used to prevent the dot product from becoming too large, or is there another reason for its inclusion?\n",
      "\n",
      "Furthermore, I'm interested in exploring the example case study you provided. You showed how the model generates text by predicting one token at a time, using the previously generated tokens as input. How does the model handle situations where there are multiple possible next tokens? For instance, in the example you provided, the model could have generated \"the kingdom of Eldrador was famous for its powerful sorcerers\" instead of \"renowned for its powerful sorcerers\". What factors influence the model's decision in such cases?\n",
      "\n",
      "Lastly, I'd love to see more examples of text generation with LLMs. Are there any specific use cases or applications where LLMs have been particularly effective? For instance, have they been used for tasks like language translation, text summarization, or content generation?\n",
      "\n",
      "I hope these questions help clarify my understanding of the concepts, and I look forward to hearing more about the applications and examples of LLMs!\n",
      "\n",
      "Teacher: I'm happy to help clarify your understanding of the concepts.\n",
      "\n",
      "### WordPiece Algorithm\n",
      "\n",
      "The WordPiece algorithm stops merging subwords when it reaches a predetermined vocabulary size. The algorithm iteratively merges the most frequent pairs of subwords until the desired vocabulary size is achieved. The vocabulary size is a hyperparameter that is set before training the model.\n",
      "\n",
      "To illustrate this, let's consider an example. Suppose we have a corpus of text with the following words: \"unbreakable\", \"unbelievable\", \"breakfast\". Initially, the vocabulary consists of individual characters: \"u\", \"n\", \"b\", \"r\", \"e\", \"a\", \"k\", \"f\", \"t\", etc. The algorithm merges the most frequent pairs of characters, such as \"un\" and \"break\", to form subwords. As the algorithm iterates, it merges subwords like \"unbreak\" and \"able\" to form \"unbreakable\". The process continues until the vocabulary size reaches the desired threshold.\n",
      "\n",
      "### \"##\" Notation in WordPiece\n",
      "\n",
      "The \"##\" notation is a predefined rule in WordPiece. When a subword is a continuation of a previous subword, it is prefixed with \"##\". This notation is not learned during training but is rather a convention used to indicate that a subword is not a standalone word.\n",
      "\n",
      "For example, if the word \"unbreakable\" is tokenized into \"un ##break ##able\", the \"##\" notation indicates that \"break\" and \"able\" are continuations of the previous subword \"un\". This notation helps the model understand the relationship between subwords.\n",
      "\n",
      "### Scaling Factor in Self-Attention Mechanism\n",
      "\n",
      "The scaling factor in the self-attention mechanism is used to prevent the dot product from becoming too large. When computing the attention weights, the dot product of Q and K can result in large values, especially when the dimensionality of the vectors is high. To mitigate this, a scaling factor of 1/√d is applied, where d is the dimensionality of the vectors.\n",
      "\n",
      "This scaling factor helps to:\n",
      "\n",
      "1.  Prevent extremely large values from dominating the softmax output.\n",
      "2.  Ensure that the attention weights are computed in a more stable and robust manner.\n",
      "\n",
      "By scaling the dot product, the model can effectively capture the relationships between different tokens in the input sequence.\n",
      "\n",
      "### Handling Multiple Possible Next Tokens\n",
      "\n",
      "When generating text, the model uses a combination of the previously generated tokens and the probability distribution over the vocabulary to determine the next token. In cases where there are multiple possible next tokens, the model's decision is influenced by several factors:\n",
      "\n",
      "1.  **Probability distribution**: The model computes a probability distribution over the vocabulary, and the next token is sampled from this distribution. The token with the highest probability is not always chosen; instead, the model may sample from the top-k or top-p tokens.\n",
      "2.  **Contextual information**: The model considers the context provided by the previously generated tokens when determining the next token.\n",
      "3.  **Training data**: The model's training data influences its predictions. If the model has been trained on a diverse range of texts, it is more likely to generate coherent and contextually relevant text.\n",
      "\n",
      "In the example you provided, the model generated \"renowned for its powerful sorcerers\" instead of \"the kingdom of Eldrador was famous for its powerful sorcerers\". This decision is based on the probability distribution over the vocabulary and the contextual information provided by the previously generated tokens.\n",
      "\n",
      "To illustrate this, let's consider a simple example. Suppose we want to generate text starting with the prompt \"The sun was shining brightly in\". The model may generate the next token as \"the\" or \"on\". The decision between these two options depends on the probability distribution over the vocabulary and the contextual information.\n",
      "\n",
      "### Applications and Examples of LLMs\n",
      "\n",
      "LLMs have been used in a variety of applications, including:\n",
      "\n",
      "1.  **Language translation**: LLMs can be fine-tuned for language translation tasks, achieving state-of-the-art results.\n",
      "2.  **Text summarization**: LLMs can summarize long documents or articles, capturing the key points and main ideas.\n",
      "3.  **Content generation**: LLMs can generate high-quality content, such as articles, stories, or even entire books.\n",
      "4.  **Chatbots and conversational AI**: LLMs can be used to build conversational AI models that engage in natural-sounding conversations.\n",
      "5.  **Sentiment analysis**: LLMs can be fine-tuned for sentiment analysis tasks, accurately detecting the sentiment of text.\n",
      "\n",
      "Some examples of LLMs in action include:\n",
      "\n",
      "*   Generating product descriptions for e-commerce websites\n",
      "*   Creating personalized content for marketing campaigns\n",
      "*   Summarizing long documents or research papers\n",
      "*   Translating text from one language to another\n",
      "*   Building conversational AI models for customer support or chatbots\n",
      "\n",
      "These are just a few examples of the many applications and use cases for LLMs. As the technology continues to evolve, we can expect to see even more innovative applications of LLMs in the future.\n",
      "\n",
      "### Additional Examples\n",
      "\n",
      "Let's consider a few more examples to illustrate the capabilities of LLMs.\n",
      "\n",
      "1.  **Generating a short story**: Given a prompt, an LLM can generate a short story. For instance, if the prompt is \"In a world where magic was real\", the LLM might generate a story like: \"In a world where magic was real, the kingdom of Eldrador was famous for its powerful sorcerers. They wielded magic with precision and accuracy, and their spells were feared throughout the land.\"\n",
      "2.  **Translating text**: LLMs can be fine-tuned for language translation tasks. For example, if we want to translate the text \"Hello, how are you?\" from English to Spanish, the LLM might generate the translation: \"Hola, ¿cómo estás?\"\n",
      "3.  **Summarizing a document**: LLMs can summarize long documents or articles. For instance, if we provide an LLM with a news article, it might generate a summary like: \"The article discusses the recent advancements in AI research, highlighting the potential benefits and challenges of developing more sophisticated AI models.\"\n",
      "\n",
      "These examples demonstrate the versatility and capabilities of LLMs in generating high-quality text, translating languages, and summarizing documents.\n",
      "\n",
      "I hope this clarifies your understanding of the concepts and provides a better insight into the applications and examples of LLMs. If you have any more questions or need further clarification, feel free to ask!\n",
      "\n",
      "Student: I think I've got a good grasp of the concepts, but I do have a few questions to clarify some things.\n",
      "\n",
      "Firstly, regarding the WordPiece algorithm, you mentioned that it stops merging subwords when it reaches a predetermined vocabulary size. How is this vocabulary size determined? Is it a fixed value that's commonly used, or does it depend on the specific task or dataset being used?\n",
      "\n",
      "Also, I'm a bit confused about the \"##\" notation in WordPiece. You said it's a predefined rule that indicates a subword is a continuation of a previous subword. But how does the model know when to apply this notation? Is it learned during training, or is it a manual process?\n",
      "\n",
      "Moving on to the self-attention mechanism, I understand that the scaling factor is used to prevent the dot product from becoming too large. But why is the scaling factor specifically 1/√d, where d is the dimensionality of the vectors? Is there a mathematical reason behind this choice, or is it an empirical finding?\n",
      "\n",
      "Lastly, I'd love to know more about how LLMs handle multiple possible next tokens during text generation. You mentioned that the model uses a combination of the probability distribution over the vocabulary and contextual information to determine the next token. But how does the model balance these two factors? Is there a specific algorithm or technique used to make this decision?\n",
      "\n",
      "Oh, and one more thing! You provided several examples of LLMs in action, such as generating product descriptions, translating text, and summarizing documents. But how do these models handle domain-specific terminology and jargon? For instance, if an LLM is generating text for a medical or technical audience, how does it ensure that the terminology is accurate and up-to-date?\n",
      "\n",
      "Teacher: I'm glad you're eager to clarify these concepts. I'll address each of your questions step by step.\n",
      "\n",
      "### Determining Vocabulary Size for WordPiece\n",
      "\n",
      "The vocabulary size for the WordPiece algorithm is typically determined based on the specific task or dataset being used. While there's no one-size-fits-all value, common vocabulary sizes range from 30,000 to 50,000 subwords for many natural language processing tasks. The choice depends on balancing the trade-off between:\n",
      "\n",
      "1. **Capturing rare or domain-specific terms**: A larger vocabulary can include more specialized vocabulary but increases the risk of overfitting to the training data.\n",
      "2. **Computational efficiency**: Larger vocabularies require more parameters and computational resources.\n",
      "\n",
      "The optimal vocabulary size is often found through experimentation and hyperparameter tuning for the specific task at hand.\n",
      "\n",
      "### Understanding the \"##\" Notation in WordPiece\n",
      "\n",
      "The \"##\" notation is a predefined rule used to indicate that a subword is a continuation of a previous subword. This notation is not learned during training but is instead applied as a preprocessing step when tokenizing input text. When the WordPiece algorithm splits a word into subwords, it appends \"##\" to the subsequent subwords to signify that they are continuations of the initial subword.\n",
      "\n",
      "For example, the word \"unbreakable\" might be tokenized into \"un##\", \"break##\", and \"able\". This notation helps the model understand that these subwords are related and part of the same original word.\n",
      "\n",
      "### The Scaling Factor in Self-Attention\n",
      "\n",
      "The scaling factor of 1/√d in the self-attention mechanism is used to prevent the dot product from growing too large as the dimensionality (d) of the vectors increases. The choice of 1/√d is based on the assumption that the vectors being dotted are independent and have a mean of 0 and a variance of 1.\n",
      "\n",
      "Mathematically, when you compute the dot product of two vectors, the variance of the result grows linearly with the dimensionality (d) of the vectors. By scaling the dot product by 1/√d, you normalize the variance to be around 1, regardless of d. This helps maintain a stable distribution of attention weights, which is crucial for the model's ability to learn effectively.\n",
      "\n",
      "This scaling factor was introduced in the Transformer architecture and has since become a standard component in many self-attention-based models.\n",
      "\n",
      "### Handling Multiple Possible Next Tokens During Text Generation\n",
      "\n",
      "When generating text, LLMs use a combination of the probability distribution over the vocabulary and contextual information to determine the next token. The model doesn't simply pick the most likely token; instead, it samples from the probability distribution, often using techniques like top-k sampling or nucleus sampling to balance between:\n",
      "\n",
      "1. **Exploiting the most likely tokens**: To generate coherent and contextually appropriate text.\n",
      "2. **Exploring less likely tokens**: To introduce diversity and creativity in the generated text.\n",
      "\n",
      "The specific algorithm or technique used can vary depending on the implementation and the desired characteristics of the generated text.\n",
      "\n",
      "### Handling Domain-Specific Terminology and Jargon\n",
      "\n",
      "LLMs can handle domain-specific terminology and jargon to a certain extent based on their training data. If the model has been trained on a large corpus that includes texts from the specific domain of interest (e.g., medical or technical texts), it is likely to learn the relevant terminology.\n",
      "\n",
      "However, ensuring that the terminology is accurate and up-to-date can be challenging, especially in rapidly evolving fields. Techniques to improve domain-specific performance include:\n",
      "\n",
      "1. **Domain-specific fine-tuning**: Further training the model on a dataset specific to the domain of interest.\n",
      "2. **Using domain-specific pre-training corpora**: Ensuring that the initial pre-training data includes a significant amount of text from the relevant domain.\n",
      "3. **Post-processing and fact-checking**: Implementing additional steps to verify the accuracy of generated text, especially in critical applications.\n",
      "\n",
      "By employing these strategies, LLMs can be adapted to handle domain-specific terminology effectively, making them more versatile and useful across a wide range of applications.\n",
      "\n",
      "Student: I'm excited to dive into these concepts, but I have to admit that some parts are still a bit unclear to me. Let's start with the WordPiece algorithm. You mentioned that the vocabulary size is typically between 30,000 to 50,000 subwords, but how do we determine the optimal size for a specific task? Is it just a matter of trial and error, or are there any specific metrics or guidelines that we can follow?\n",
      "\n",
      "Also, I'm intrigued by the \"##\" notation used to indicate that a subword is a continuation of a previous subword. Can you provide more examples of how this notation is used in practice? For instance, how would the word \"unbreakable\" be tokenized, and what's the significance of the \"##\" notation in this context?\n",
      "\n",
      "Moving on to the self-attention mechanism, I think I understand the concept of the scaling factor, but I'd love to see a more detailed mathematical explanation. How exactly does the variance of the dot product grow linearly with the dimensionality of the vectors, and why is it necessary to scale the dot product by 1/√d?\n",
      "\n",
      "Lastly, I'm curious about how LLMs handle multiple possible next tokens during text generation. You mentioned that the model uses a combination of the probability distribution over the vocabulary and contextual information to determine the next token. Can you elaborate on how this process works, and what are the trade-offs between exploiting the most likely tokens and exploring less likely tokens?\n",
      "\n",
      "Oh, and one more thing! How do LLMs handle domain-specific terminology and jargon? You mentioned that the model can learn relevant terminology based on its training data, but what if the domain is highly specialized or rapidly evolving? Are there any specific techniques or strategies that can be used to improve the model's performance in these cases?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Personalities\n",
    "teacher_intro = SystemMessage(\n",
    "    content=\"You are a knowledgeable and patient TEACHER. \"\n",
    "            \"Explain concepts clearly, give examples, and guide the student step by step.\"\n",
    ")\n",
    "\n",
    "student_intro = SystemMessage(\n",
    "    content=\"You are a curious STUDENT. \"\n",
    "            \"Ask thoughtful questions, sometimes express confusion, and seek clarification.\"\n",
    ")\n",
    "\n",
    "# Initialize conversation\n",
    "conversation_log = []\n",
    "message = \"Hello teacher, I’m ready to learn about Large Language Models.\"  # student starts\n",
    "\n",
    "print(\"Start of Teacher–Student Conversation:\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    if i % 2 == 0:\n",
    "        # Teacher response\n",
    "        response = model1.invoke([teacher_intro, HumanMessage(content=message)])\n",
    "        print(f\"Teacher: {response.content}\\n\")\n",
    "        conversation_log.append((\"Teacher\", response.content))\n",
    "        message = response.content  # Passing teacher's answer to student\n",
    "    else:\n",
    "        # Student response\n",
    "        response = model2.invoke([student_intro, HumanMessage(content=message)])\n",
    "        print(f\"Student: {response.content}\\n\")\n",
    "        conversation_log.append((\"Student\", response.content))\n",
    "        message = response.content  # Passing student's reply back to teacher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522fe94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroToLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
